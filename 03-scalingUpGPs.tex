\chapter{Scaling up Gaussian Process Regression}
\label{chapScalingGPR}

The GP regression approach as mentioned in earlier chapter is intractable for large data sets. For a data set of size \(N\) the covariance matrix \(K_{XX}\) is of size \(N \times N\),  where \(\mathcal{O}\left ( N^{3} \right )\) time is needed for calculating the precision matrix and \(\mathcal{O}\left ( N^{2} \right )\) memory for storage. Since, inverting the covariance matrix takes considerable amount of time and memory, almost all techniques to scale up GP regression try to approximate the inversion of Gram matrix \(K_{XX}\). 

Let us take the example of an SE kernel, for a large value of length-scale the Gram matrix (\(K_{XX}\)) is spread out and has a rank lower than  \(N\) (figure \ref{subFigSEPrior_2}). Due to this low-rank characteristic the Gram matrix can be approximated as a lower rank form reducing the cost of inverting the Gram matrix. In the GP literature sparse approximations (section \ref{secSparseApprox}) use a set of inducing points to compress the information of the several observations through the low-rank approximation. 

For the same SE kernel if the length-scale tends to a low value the Gram matrix is not of low-rank but tends to a diagonal matrix (figure \ref{subFigSEPrior_1}). In the GP literature the mixture of experts (section \ref{secDgp}) methodology exploits the block diagonal nature of the Gram matrix by distributing data points into a subset of experts, assuming independence across experts and distributing the calculations into several batches. The first regime suggests global (numerical) low-rank approximations while the second regime suggests local block-diagonal approximations \cite{march2015askit, chenhan2016inv}. 

The remaining chapter unfolds as follows, section \ref{secSparseApprox} describes the Sparse Approximations detailing several methods of choosing inducing points and then performing experiments on a toy dataset. Section \ref{secDgp} describes the Distributed GP methodology detailing several methods for merging of experts and then performing experiments on the same toy-dataset. 

\section{Sparse Approximations}\label{secSparseApprox}
Sparse methods use a small subset of input points as support or inducing points to approximate the Gram matrix. Suppose we use \(M\) inducing points \(X^{m} = \{x^{m}_{1}; x^{m}_{2}; \ldots; x^{m}_{M}\}\), such that \(M < N\). The points \(X^{m}\) can be a subset of training inputs in the input space. 


\subsection{Nystr\"{o}m Approximation}\label{subSecNystrom} 

Using Nystr\"{o}m approximation the Gram matrix can be approximated as equation \ref{eqnSparseNystormGram} \textbf{add more detail here as well}. Refer to \cite{quinonero2005unifying, seeger2003fast} for more detail. 

\begin{equation}\label{eqnSparseNystormGram}
K_{nystorm}(X, X) = K(X, X^{m})K(X^{m}, X^{m})^{-1}K(X^{m}, X)
\end{equation}

Here, \(K(X^{m}, X^{m})\) is a \(M \times M\) Gram matrix evaluated at inducing points \(X^{m}\), \(K(X, X^{m})\) is an \(N \times M\) Gram matrix between training points and inducing points. The inversion of approximate matrix takes \(\mathcal{O}\left ( NM^{2} \right )\) time to compute. Figure \ref{subFigNystormSEmatrix} is an approximate Gram matrix using Nystr\"{o}m approximation of the matrix in figure \ref{subFigcovSEmatrix_1} at the input points \(X^{*} = \{[0:0.02:1]\}\). The inducing points \(X^{m}\) were chosen randomly from the set of input points and their location is denoted by white lines. We can observe that if the gap between inducing points increases then accuracy of Gram matrix degrades (eg. at \(x \sim 0.5\)). Figure \ref{subFignystormSEmatrixUniform} is again an approximated Gram matrix using Nystr\"{o}m approximation of the matrix in figure \ref{subFigcovSEmatrix_1}. This time the equally spaced inducing points are chosen in the range of \(X^{*}\). Notice the significant improvement in the Gram matrix upon different set of inducing inputs.

\begin{figure}[!ht]
  \centering
    \subfigure[{Approximated Gram matrix using Nystr\"{o}m approximation for a Standard Exponential (SE) Kernel with \((\theta = [1, 0.2])\) (figure \ref{subFigcovSEmatrix_1}) at the input points \(X^{*} = \{[0:0.02:1]\}\). The inducing points were chosen randomly, the white lines denote the location of inducing points. }]
  {
        \includegraphics[width=0.45\textwidth]
        {images/nystormSEmatrix}
        \label{subFigNystormSEmatrix}
  }\quad
\subfigure[{Approximated Gram matrix using Nystr\"{o}m approximation for a Standard Exponential (SE) Kernel with \((\theta = [1, 0.2])\) (figure \ref{subFigcovSEmatrix_1}) at the input points \(X^{*} = \{[0:0.02:1]\}\). The white lines denote the location of inducing points, the inducing points are uniformly distributed. Notice the significant improvement in Gram matrix due to different inducing inputs}]
  {
        \includegraphics[width=0.45\textwidth]
        {images/nystormSEmatrixUniform}
        \label{subFignystormSEmatrixUniform}
  }\quad
  
       \caption{Approximate Gram matrix for a Standard Exponential kernel using Nystr\"{o}m approximation.}\label{figGPNystormGramMatrix}
\end{figure}

Later, \cite{Snelson06sparsegaussian} proposed the FITC approach which corrects the diagonal terms of the Gram matrix and improves the prediction capabilities (equation \ref{eqnSparseNystormGram}).

\begin{equation}\label{eqnSparseFITCGram}
K_{FITC}(X, X) = diag[K(X, X) - K_{nystorm}(X, X)] + K_{nystorm}(X, X)
\end{equation}

Note, calculating \(diag(K(X, X))\) is an \(\mathcal{O}\left ( N \right )\) operation and thus does not significantly impact the time taken. 


The posterior distribution for the approximate prior can be derived similarly as given in appendix \textbf{add appendix ref} and is a Gaussian. The predictive mean and predictive variance are written as equation \ref{eqNoisyNystormPredictiveMean} and equation \ref{eqNoisyNystormPredictiveCovariance}. Here, \(K_{approximate}(X, X')\) can be the approximated Gram matrix either from the Nystr\"{o}m approximation (equation \ref{eqnSparseNystormGram}) or the FITC approximation (equation \ref{eqnSparseFITCGram}). 
\begin{equation}\label{eqNoisyNystormPredictiveMean}
  \mathbf{E}[f_{approximate}(x_{*})] = K_{Xx_{*}}^{T}( K_{approximate}(X, X') + \sigma^{2}_{n}I)^{-1}Y
  \end{equation}
\begin{equation}\label{eqNoisyNystormPredictiveCovariance}
	Cov[f_{approximate}(x_{*})] = K_{x_{*}x_{*}} - K_{Xx_{*}}^{T}( K_{approximate}(X, X') + \sigma^{2}_{n}I )^{-1} K_{Xx_{*}}
  \end{equation}


By approximating the \(K(X, X)\) using the inducing points we have effectively changed the GP prior. This means that \(X^{m}\) have also become the hyper-parameters of our GP prior. Hence, we should fine-tune locations of \(X^{m}\) and the hyper-parameters \(\theta\) to obtain a good prediction of our data. The marginal likelihood for the approximate prior (equation \ref{equationApproximateML}) be written similarly as equation \ref{equationMarginalLikelihood}.

\begin{equation}\label{equationApproximateML}
    \Pr[Y(X) \mid X, X^{m}, \theta, \sigma_{n}] = \mathcal{N}(0 , K_{approximate}(X, X') + \sigma^{2}_{n}I)
\end{equation}


The maximization of the marginal likelihood in equation \ref{equationApproximateML} with respect to (\(X^{m}\); \(\theta\)), is prone to over-fitting especially when the number of inducing inputs is large. This means that if we keep on increasing the number of inducing points a time will come when we will tend to decrease the accuracy of our predictions on the test data set. The variational approximation (detailed next) approach overcomes this issue of over-fitting by adding a regularization term penalizing over-fitting.

\subsection{Variational Approximation}\label{subSecVariationalApprox} 
The variational approximation does not attempt to approximate the Gram matrix. Instead, it assumes a probability distribution \(q(f)\) of the true posterior distribution \(p(f \mid y)\) and minimizes the distance between the two \cite{Titsias09variationallearning}. 

The \(q(f)\) is written in terms of inducing points (\(X^{m}\)) and the KL divergence \(KL(q||p)\)\footnote{KL divergence is a measure of distance between two probability distributions} is minimized between the variational distribution \(q\) and true distribution \(p\). When we minimize the KL divergence we are making the assumed distribution closer to true distribution and hence improving the values of (\(X^{m}\)) and \(\theta \). This minimization of KL divergence is equivalently expressed as the maximization of the equation \ref{equationLowerBoundVarNLML}

\begin{equation}\label{equationLowerBoundVarNLML}
F_{V} = log(\mathcal{N}[0, \sigma_{n}^{2}I + K_{nystorm}(X, X)]) - \frac{1}{2\sigma_{n}^{2}}Tr(K_{XX} - K_{nystorm}(X, X))
\end{equation}

Notice, the similarity between equation \ref{equationLowerBoundVarNLML} and \ref{eqnSparseNystormGram}. The novelty of the above objective function is that it contains a regularization term: \(- \frac{1}{2\sigma ^{2}}Tr(K_{XX} - K_{nystorm}(X, X))\). Thus, \(F_{V}\) attempts to maximize the marginal likelihood as derived for Nystr\"{o}m approximation and simultaneously minimizes the trace. When the regularization term tends to zero \(K_{XX} - K_{nystorm}(X, X)\), which means that the inducing variables can exactly reproduce the full GP prediction. 

The posterior distribution for variational inference approximation is same as the one derived for Nystr\"{o}m approximation. The difference between Nystr\"{o}m approximation and variational approximation is the improvement in the evaluation parameter while optimizing \(X^{m}\) and \(\theta\). Due to the additional trace term variational inference reduces over-fitting.

\subsection{Experiments}\label{subsecNystromExperiments}
We here conduct experiments on a toy-data set to observe the accuracy of Nystr\"{o}m approximation for varying number and location of inducing points. The basic toolbox used for this paper is GPML provided with \cite{rasmussen2006gaussian} on MATLAB 2014b. All experiments were performed on an Intel quad-core processor with 4Gb RAM. 

10-fold Cross Validation (CV) will be used to assess the performance of the prediction. CV is a technique where the dataset is partitioned as the test set and training set. A model is learned using the training set and Root Mean Square Error (RMSE) is calculated between the prediction and test set as a measure of accuracy. In the 10-fold version of CV, the dataset will be randomly partitioned into 10 subsets containing an equal number of points. Of the 10 subsets, a single subset is retained as the test dataset, and the remaining 9 (10 - 1) subsets are used as training data. The cross-validation process is then repeated 10 times (the folds), with each of the k subsets used exactly once as the validation data.

The toy data set was generated at 1000 input points \(X = \{[-1:0.002:1]\}\) by sampling a random function from a GP\footnote{\((\Pr[Y \mid X, \theta, \sigma_{n}] = GP(0, K_{SE}(X, X', \theta = [1, 0.1]) + (0.3)^{2}I)\)} with zero mean, SE covariance function (\(\theta = [1, 0.1]\)) and noise \(\sigma_{n} = 0.3\). 

Figure \ref{predictionOfm10_242} is the prediction of the GP obtained after Nystr\"{o}m approximation using 10 inducing points. The solid black line defines the mean function, blue region defines 95\% confidence interval (2\(\sigma\)) distance away from the mean. The points denoted by `+' sign are initial locations of inducing points, while the points denoted by `*' sign are locations of inducing points after optimization. The points denoted by `.' are the test points for this fold of the 10-fold CV. 

Notice the inducing points, while initially randomly distributed are later uniformly distributed due to optimization of marginal likelihood. In this case, since the training points are randomly distributed, uniformly distributed inducing inputs are better approximations of the Gram matrix. In cases where training data set tends to be dense in one region and sparse in another region, lesser inducing points get allocated at the dense region and more get allocated at the sparse region. This happens because the information contained in a dense cluster of data-points can be approximated by a fewer data points (points in a neighbourhood are similar (section \ref{subSecCH2Covariance}) \cite{Snelson06sparsegaussian}.

Figure \ref{boxPlotsOfPerformance_242} are 10-fold RMSE box-plots for varying number of inducing points. The box-plots in red are cases when only the hyper-parameters were optimized while inducing inputs were distributed randomly.The box-plots in blue are the cases when both locations of inducing points and hyper-parameters are optimized. The accuracy of prediction improves with increasing number of inducing points. Accuracy is generally better when both locations of inducing points and hyper-parameters are optimized. Note, the noise in the generated toy-data is \(\sigma_{n}=0.3\), hence \(0.3\) is the best achievable RMSE value. Models constructed when optimizing both \(\theta, X^{m}\) reach this RMSE limit for \(M = 20\). After \(M=50\) accuracy is similar for both the optimization routines. As a thumb rule if \(M = \frac{N}{10}\) then randomly distributing the inducing points and optimizing \(\theta\) will be sufficient to give a good prediction \cite{cao2013efficient}. 

\begin{figure}[!ht]
  \centering
    \subfigure[{Posterior between a Nystorm approximated SE prior with 10 inducing inputs and training data. The solid black line defines the mean function, blue region defines 95\% confidence interval (2\(\sigma\)) distance away from the mean. The points denoted by `+' sign are initial locations of inducing points, while the points denoted by `*' sign are locations of inducing points after optimization.}]
  {
        \includegraphics[width=0.45\textwidth]
        {images/predictionOfm10_242}
        \label{predictionOfm10_242}
  }\quad
\subfigure[{10-fold RMSE box-plots for varying number of inducing points. The box-plots in red are cases when only the hyper-parameters \(\theta\) were optimized while inducing inputs were distributed randomly. The box-plots in blue are the cases when both locations of inducing points \(X^{m}\) and hyper-parameters \(\theta\) are optimized. }]
  {
        \includegraphics[width=0.45\textwidth]
        {images/boxPlotsOfPerformance_242}
        \label{boxPlotsOfPerformance_242}
  }\quad
  
       \caption{Results of Nystr\"{o}m Approximation on a toy-data set of size \(N=1000\) }\label{figGPPredictionNystorm}
\end{figure}

Global low-rank approximations are best suited for the case of spread out Gram matrices (example high length-scale SE priors). We have seen three types of low-rank approximation algorithms in this section. While Nystr\"{o}m and FITC approximations are the simplest method to approximate Gram matrix, finding optimal locations of the inducing points can often lead to over-fitting. We then, look at variational approximation procedure which adds a regularization term while finding inducing points thereby penalizing over-fitting. The lower computational cost due to sparse approximations, scales sparse GPs to the training set of sizes \(N \sim \mathcal{O}(5 \times 10^5)\). \cite{Gal2014Distributed} propose a distributed architecture for scaling variational sparse GPs. In the next section we look at how to approximate Gram matrices using mixture of experts, this enables us to massively scale GPs to sizes \(N > \mathcal{O}(10^6)\) by exploiting distributed architecture.

\section{Distributed Gaussian Process}\label{secDgp}
In the year 2006 Netflix prize was launched, teams from all over the world competed in the competition to make the best video recommendation algorithm. As the competition progressed teams figured out that performance increases upon combining algorithms developed by multiple teams. The winner and the runner-up were stacked learners of over 100 algorithms. Creating model ensembles (also called mixture of experts) is now a standard practice in many learning competitions \cite{bauer1998empirical}. 

Mixture of experts methods in GPs use bagging, where subsets of data are generated, individual GPs are trained on these subsets and their results are finally combined \cite{chen2009bagging}. If the data set is partitioned  into \(N_{experts}\) subsets such as $\mathcal{D}^{(i)} = {X^{(i)}, Y^{(i)}}, i \in 1, \ldots N_{experts}$. Each subset of data learns an individual GP model, which can be combined together to give final predictions . Due to individual learning choosing hyper-parameters and calculating prediction become easily parallel-able and indifferent to the computational infrastructure. 

Initially, this mixture of local models was used to highlight local features in the data \cite{rasmussen2002infinite}. \cite{ng2014hierarchical} propose to use the mixture of experts methodology to speed up prediction in a Gaussian Process. Instead of learning a different GP for each subset, we tie all the different experts using one single set of hyperparameters. This is equivalent to assuming one single GP for the whole data-set such that there is no correlation across experts, i.e. the experts are independent of each other. This tying of experts greatly reduces the number of hyper-parameters to optimize, acts as a regularization and inhibits over-fitting. Equation \ref{distributedGPPrior} denotes an independent GP prior for each expert \(\mathcal{D}^{(i)}\) such that the hyper-parameters \(\theta\) and \(\sigma_{n}\) are same for all experts.

\begin{equation}\label{distributedGPPrior}
    \Pr[y^{(i)} \mid x^{(i)}, \theta, \sigma_{n}] = GP(0, K(x^{(i)}, x^{(i)'}, \theta) + \sigma^{2}_{n}I) 
\end{equation}

Figure \ref{subFigdistributedKernelRandomExperts} is an approximate Gram matrix using distributed GP approximation for a Standard Exponential (SE) Kernel with \((\theta = [1, 0.2])\) (figure \ref{subFigcovSEmatrix_1}) at the input points \(X^{*} = \{[0:0.02:1]\}\). 5 experts each having 100 points are chosen, points in the experts are distributed randomly, this gives the approximate Gram matrix scattered shape. Figure \ref{subFigDistributedKernel} is an approximate Gram matrix using distributed GP approximation of the matrix in figure \ref{subFigcovSEmatrix_1} and uniformly distributed experts. 5 experts each having 100 points are chosen, The first expert has first set of 100 points, the second expert has the second set of 100 points and so on. The Gram matrix with randomly chosen experts has a more global nature but lacks many high variance regions. The Gram matrix for uniformly chosen experts retains more local features. Inversion of this Gram matrix is an operation of complexity \(\mathcal{O}(N_{experts}P^{3})\), where \(P\) is the number of points in an expert.

\begin{figure}[!ht]
  \centering
    \subfigure[{Approximated Gram matrix using distributed GP approximation for a Standard Exponential (SE) Kernel with \((\theta = [1, 0.2])\) (figure \ref{subFigcovSEmatrix_1}) at the input points \(X^{*} = \{[0:0.02:1]\}\). Points in the experts are distributed randomly, this gives the approximate Gram matrix scattered shape.}]
  {
        \includegraphics[width=0.45\textwidth]
        {images/distributedKernelRandomExperts}
        \label{subFigdistributedKernelRandomExperts}
  }\quad
\subfigure[{Approximated Gram matrix using distributed GP approximation for a Standard Exponential (SE) Kernel with \((\theta = [1, 0.2])\) (figure \ref{subFigcovSEmatrix_1}) at the input points \(X^{*} = \{[0:0.02:1]\}\). Points in the experts are distributed uniformly. We can observe that covariance across experts goes to zero.}]
  {
        \includegraphics[width=0.45\textwidth]
        {images/distributedKernel}
        \label{subFigDistributedKernel}
  }\quad
  
       \caption{Approximate Gram matrix for a Standard Exponential kernel using mixture of experts.}\label{figGPApproximateDGPMatrix}
\end{figure}

Since the experts are independent of each other, we can construct a posterior distribution for each expert (equations \ref{eqPredictiveMeanIndividualExpert} and \ref{eqPredictiveCovarianceIndividualExpert}). In the following equations \(m^{(i)}(x_{*})\) and \(\sigma^{(i)}(x_{*})\) are the mean and covariance predictions from expert \(i\) at point \(x_{*}\).  

\begin{equation}\label{eqPredictiveMeanIndividualExpert}
  m^{(i)}(y(x_{*})) = K_{x^{(i)}x_{*}}^{T}( K_{x^{(i)}, x^{(i)'}} + \sigma^{2}_{n}I)^{-1}y^{(i)}
  \end{equation}
\begin{equation}\label{eqPredictiveCovarianceIndividualExpert}
	\sigma^{(i)}(y(x_{*})) = K_{x_{*}x_{*}} - K_{x^{(i)}x_{*}}^{T}( K_{x^{(i)}, x^{(i)'}} + \sigma^{2}_{n}I )^{-1} K_{x^{(i)}x_{*}}
  \end{equation}

\subsection{Combining experts}\label{subSecCombiningExperts}
There exist several methods in the literature on how to combine these individual posterior predictions of the experts to give the final posterior distribution. The Product of Experts model uses the independence assumption between experts and multiplies the individual posterior distribution\footnote{\(\Pr[y(x_{*}) \mid x_{*}, \mathcal{D}, \theta] \propto \prod \Pr[y^{(i)} \mid x^{(i)}, \mathcal{D}^{(i)}, \theta]\)}, but these predictions tend to be overconfident. Another method called the generalized Product of Experts (gPOE) assigns a participation factor to each expert based on the amount of uncertainty in prediction (more confident experts have higher say in prediction)\cite{cao2014generalized}. The Bayesian Committee Machine (BCM) imposes the independence assumption between each expert pair using Bayes Rule, but can result in bad predictions when leaving data regime \cite{tresp2000bayesian}. 

This thesis will use robust Bayesian Committee Machine (rBCM) model to combine the posterior distributions of experts \cite{deisenroth2015distributed}. The rBCM model is an amalgamation of all the above three mentioned methods, it combines the confidence weighting parameter of gPOE with the Bayesian formulation in BCM technique to generate the following posterior distributions.

\begin{equation}\label{eqCovarianceDGP}
    Cov(y(x_{*}))^{^-2} = \sum_{i} \beta_{i}\sigma_{(i)}^{-2} + (1- \sum_{i} \beta_{i})(K_{x_{*}x_{*}})^{-2}
\end{equation}
\begin{equation}\label{eqMeanDGP}
    m(y(x_{*})) = (Cov(y(x_{*})))^{-2}\sum_{i} \beta_{i}(\sigma^{(i)})^{-2}m^{(i)}(x_{*})
\end{equation}

\(K_{x_{*}x_{*}}\) is the auto-covariance of the prior at prediction point \(x_{*}\). \(\beta_{k}\) determines the influence of experts on the final predictions \cite{cao2014generalized} and is given as \(\beta_{i} = \frac{1}{2}(\log K_{x_{*}x_{*}}^{2} - \log(\sigma^{(i)})^{2})\). Experts which are very confident of their predictions at \(x_{*}\) will tend to have low \(\sigma^{(i)}\) thereby leading to a higher influence factor \(\beta_{i}\).

Due to the independence assumption, the marginal likelihood can be written as a sum of individual likelihoods and then can be optimized to find the best-fit hyperparameters. By approximating the \(K(X, X)\) in terms of \(\mathcal{D}^{(i)}\) and \(N_{experts}\) we have again changed the GP prior. This means that the number of experts \(N_{experts}\) and clustering of points in individual experts also impact the prediction capabilities of GP. The below equation \ref{eqDGPNLML} describes the formulation for marginal likelihood. 

\begin{align}\label{eqDGPNLML}
    \log \Pr[y \mid X, \mathcal{D}, \theta] \approx \sum_{k=1}^{N_{experts}} \log \Pr[y^{(i)}\mid X^{(i)}, \theta]
 \end{align}

Maximizing the above log-marginal likelihood will give the optimal values of hyper-parameters. Points in the experts can be distributed either randomly or using a clustering scheme (eg. k-means clustering\footnote{k-means algorithm clusters close by points in one cluster. The notion of closeness is defined by some measure of distance}) for stationary kernels\footnote{stationary kernels are only a function of \(\tau = |x-x'|\)} k-means clustering should be preferred. The k-means algorithm clusters points based on a measure of distance, points in separate clusters are far away from each other when compared to points in the same cluster. This means that the covariance (for stationary kernels) between separate clusters is significantly lower when compared to points in same cluster. Hence, the cross-covariance across separate clusters can be more easily assumed to be zero.

\subsection{Experiments}\label{subSecDistributedExperiments}
We again conduct experiments on a toy-data set, this time to observe the accuracy of distributed GP approximation for varying number of experts. 

Again the 10-fold Cross Validation (CV) will be used to assess the performance of the prediction. The same toy-data set as used in section \ref{subsecNystromExperiments} was used to perform the experiments in this section (1000 data-points from \(\Pr[y \mid X, \theta, \sigma_{n}] = GP(0, K_{SE}(X, X', \theta = [1, 0.1]) + (0.3)^{2}I)\).

Figure \ref{predictionOfm10_243} is the prediction of the GP obtained after distributed approximation using 9 experts and k-means clustering. The solid black line defines the mean function, blue region defines 95\% confidence interval (2\(\sigma\)) distance away from the mean. The colored points in the points denoted by `*' at the bottom show how different points are distributed across experts, similar colored points belong to one expert. The data denoted by `.' is the test data for one fold of the 10-fold CV. 

The points across experts are uniformly distributed, as can be observed by the coloring scheme. Since the training points are almost uniformly distributed, the k-means algorithm will cluster the points uniformly. Actually, the training and test set used in figures \ref{predictionOfm10_243} and \ref{predictionOfm10_242} are same. Notice how Nystr\"{o}m approximation has a global smooth shape while the distributed GP approximation retains the local features of the data set. 

Figure \ref{boxPlotsOfPerformance_243} are 10-fold RMSE box-plots for different number of \(P\). The box-plots in red are cases when the points are distributed using k-means clustering. The box-plots in blue are the cases when points are distributed randomly. The accuracy of prediction improves with increasing number of points in an expert. Note, the noise in the generated toy-data is \(\sigma_{n}=0.3\), hence \(0.3\) is the best achievable RMSE value. Accuracy is slightly better when experts are distributed using k-means clustering, both being very close to the \(0.3\) RMSE limit. As a thumb-rule setting \(P = N/10\) and optimizing the hyper-parameters (\(\theta\)) is a good enough approximation. 


\begin{figure}[!ht]
  \centering
    \subfigure[{Prediction of the GP obtained after distributed approximation using 9 experts and k-means clustering. The solid black line defines the mean function, blue region defines 95\% confidence interval (2\(\sigma\)) distance away from the mean. The colored points in the points denoted by `*' at the bottom show how different points are distributed across experts, similar colored points belong to one expert. The data denoted by `.' is the test data for one fold of the 10-fold CV. }]
  {
        \includegraphics[width=0.45\textwidth]
        {images/predictionOfm10_243}
        \label{predictionOfm10_243}
  }\quad
\subfigure[{10-fold RMSE box-plots for different number of points across. The box-plots in red are cases when only the hyper-parameters when the points are distributed using k-means clustering. The box-plots in blue are the cases when points are distributed randomly. The accuracy of prediction improves with increasing number of points in an expert}]
  {
        \includegraphics[width=0.45\textwidth]
        {images/boxPlotsOfPerformance_243}
        \label{boxPlotsOfPerformance_243}
  }\quad
  
       \caption{Results of distributed GP Approximation on a toy-data set of size \(N= 1000\)}\label{figGPPredictionDistributed}
\end{figure}

\section{Discussion}
The calculation of posterior distribution in GPs becomes computationally intractable for large data sets. Calculating the precision matrix is an operation of computational complexity \(\mathcal{O}(N^{3})\), putting a limit of \(N \sim 10^4\) data points for model building. This chapter describes the state of art for scaling up GPs for regression tasks. There exist two methods to scale up GPs, the first called sparse methods which uses a set of inducing inputs to reduce the computational cost of calculating the precision matrix. The second called distributed GP which divides the data set into another set of smaller datasets called experts, distributing the model building into several batches. 

Sparse methods use Nystr\"{o}m approximation rewriting the Gram matrix as equation \ref{subSecSamplingFunctionsGPPrior}, thereby reducing the computational complexity to \(\mathcal{O}(NM^{2})\) (M << N), \(M\) being the number of inducing points. Through experiments on a toy-dataset it can be shown that we can set \(M \sim N/10\) for randomly distributed inducing points and \(M \sim N/50\) when we optimize the locations of inducing points. This approximation pushes the limit of GP Regression to \(N \sim 10^6\) data points. Distributed GPs distributes the GP Regression tasks into several batches, thereby reducing the computational complexity to \(\mathcal{O}(NP^{3})\) (P<<N), \(P\) being the number of points in an expert. Through experiments on a toy-dataset we demonstrate that \(P \sim N/100\) does not effects the regression task significantly. In fact we can further reduce \(P\) if we enable repetition of points between experts. This enables to scale GPs to any number of data-points, we will demonstrate this by running a GP regression on millions of data-points in this manuscript (section \ref{chapCombiningBasicCovariances}). 

There are several reasons why GPs should be preferred to perform regression tasks. GPs provide a probabilistic framework to define a family of functions, while the covariance functions allows to incorporate a wide range of assumptions (Chapter \ref{chapBasicCovarianceKernels}). GPs are computationally tractable, given a covariance function and observations, the predictive distribution can be calculated exactly. By providing a closed form expression of marginal likelihood GPs provide a powerful method to automatically select hyperparameters. Although GPs suffer in presence of large data sets, there exist several approximate methods to scale GPs to millions of data points. 




